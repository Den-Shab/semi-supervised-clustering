{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Распаковка архива"
      ],
      "metadata": {
        "id": "dHqhTGp-iqYI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZjQlG1bFtYN",
        "outputId": "d218226a-b50d-4d21-990c-af7da99f6e5d"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/caltech-101.zip'\n",
        "!tar -xvzf '/content/caltech-101/101_ObjectCategories.tar.gz'\n",
        "!tar -xvf '/content/caltech-101/Annotations.tar'\n",
        "!mkdir '/content/caltech101'\n",
        "!cp -r '/content/101_ObjectCategories/' '/content/caltech101'\n",
        "!cp -r '/content/Annotations/' '/content/caltech101'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импортирование необходимых библиотек и функций"
      ],
      "metadata": {
        "id": "GsOEUGWQi0EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.cluster import KMeans, SpectralClustering\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import networkx as nx\n",
        "from collections import defaultdict\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.metrics.pairwise import rbf_kernel, pairwise_distances\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.utils.validation import check_array\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from scipy.spatial.distance import squareform\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from scipy.sparse.csgraph import laplacian\n",
        "from scipy.linalg import eigh\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "\n",
        "import random\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "dSVaAEuOi9Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Дополнительные методы"
      ],
      "metadata": {
        "id": "om9xCrp-jDuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_constraints(labels, labeled_indices):\n",
        "    must_link = []\n",
        "    cannot_link = []\n",
        "    known_labels = {}\n",
        "    for idx in labeled_indices:\n",
        "        known_labels[idx] = labels[idx]\n",
        "\n",
        "    for i in labeled_indices:\n",
        "        for j in labeled_indices:\n",
        "            if i < j:\n",
        "                if labels[i] == labels[j]:\n",
        "                    must_link.append((i, j))\n",
        "                else:\n",
        "                    cannot_link.append((i, j))\n",
        "    return must_link, cannot_link, known_labels\n",
        "\n",
        "\n",
        "def evaluate_clustering(labels_true, labels_pred):\n",
        "    ari = adjusted_rand_score(labels_true, labels_pred)\n",
        "    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n",
        "    fm = fowlkes_mallows_score(labels_true, labels_pred)\n",
        "    return ari, nmi, fm\n",
        "\n",
        "\n",
        "def load_caltech101(data_path='/content/dataset/',\n",
        "                    subset=None,\n",
        "                    download=True,\n",
        "                    n_samples=2000):\n",
        "\n",
        "    transform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                    transforms.Lambda(lambda x: x.convert('RGB')),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    caltech101_dataset = datasets.Caltech101(root=data_path,\n",
        "                                             download=False,\n",
        "                                             transform=transform,\n",
        "                                             target_transform=transforms.Lambda(lambda y: torch.tensor(y, dtype=torch.long)))\n",
        "\n",
        "    total_samples = len(caltech101_dataset)\n",
        "    n_samples = min(n_samples, total_samples)\n",
        "    indices = random.sample(range(total_samples), n_samples)\n",
        "    from torchvision.models import resnet18\n",
        "    model = resnet18(pretrained=True).eval()\n",
        "    data = []\n",
        "    labels = []\n",
        "    for idx in indices:\n",
        "        img, target = caltech101_dataset[idx]\n",
        "        features = model(torch.tensor(img[np.newaxis, :])).detach().numpy()\n",
        "        data.append(features.flatten())\n",
        "        labels.append(target.item())\n",
        "\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "mQmu7zQbjC9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KMeans"
      ],
      "metadata": {
        "id": "TeyR2Eufjk9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans с прямым учетом ограничений"
      ],
      "metadata": {
        "id": "xIM_TQC6lsug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConstrainedKMeans:\n",
        "    def __init__(self, n_clusters, must_link=[], cannot_link=[], init='k-means++', n_init=10, max_iter=1000, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.must_link = must_link\n",
        "        self.cannot_link = cannot_link\n",
        "        self.init = init\n",
        "        self.n_init = n_init\n",
        "        self.max_iter = max_iter\n",
        "        self.random_state = random_state\n",
        "        self.cluster_centers_ = None\n",
        "        self.labels_ = None\n",
        "        self.build_cannot_link_dict()\n",
        "\n",
        "    def build_must_link_components(self, n_samples):\n",
        "        if not self.must_link:\n",
        "            return np.arange(n_samples)\n",
        "        rows, cols = zip(*self.must_link)\n",
        "        data = np.ones(len(rows), dtype=int)\n",
        "        graph = csr_matrix((data, (rows, cols)), shape=(n_samples, n_samples))\n",
        "        graph = graph + graph.T + csr_matrix(np.eye(n_samples))\n",
        "        _, labels = connected_components(graph)\n",
        "        return labels\n",
        "\n",
        "    def build_cannot_link_dict(self):\n",
        "        self.cannot_link_dict = defaultdict(set)\n",
        "        for i, j in self.cannot_link:\n",
        "            self.cannot_link_dict[i].add(j)\n",
        "            self.cannot_link_dict[j].add(i)\n",
        "\n",
        "    def fit(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        must_link_components = self.build_must_link_components(n_samples)\n",
        "        unique_components = np.unique(must_link_components)\n",
        "        n_components = len(unique_components)\n",
        "        component_to_indices = defaultdict(list)\n",
        "        for idx, comp in enumerate(must_link_components):\n",
        "            component_to_indices[comp].append(idx)\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, init=self.init, n_init=self.n_init, max_iter=1, random_state=self.random_state)\n",
        "        kmeans.fit(X)\n",
        "        initial_centroids = kmeans.cluster_centers_\n",
        "        component_to_cluster = {}\n",
        "        for comp in unique_components:\n",
        "            indices = component_to_indices[comp]\n",
        "            comp_points = X[indices]\n",
        "            mean_point = np.mean(comp_points, axis=0)\n",
        "            closest_cluster = np.argmin(np.linalg.norm(initial_centroids - mean_point, axis=1))\n",
        "            component_to_cluster[comp] = closest_cluster\n",
        "        self.labels_ = np.zeros(n_samples, dtype=int)\n",
        "        for comp, indices in component_to_indices.items():\n",
        "            self.labels_[indices] = component_to_cluster[comp]\n",
        "        self.cluster_centers_ = initial_centroids.copy()\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            centroids = np.zeros((self.n_clusters, n_features))\n",
        "            counts = np.bincount(self.labels_, minlength=self.n_clusters)\n",
        "\n",
        "            for k in range(self.n_clusters):\n",
        "                if counts[k] > 0:\n",
        "                    centroids[k] = X[self.labels_ == k].mean(axis=0)\n",
        "            distances_cache = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
        "            new_labels = self.labels_.copy()\n",
        "            changed = False\n",
        "\n",
        "            for comp, indices in component_to_indices.items():\n",
        "                current_cluster = new_labels[indices[0]]\n",
        "                has_conflict = False\n",
        "                for idx in indices:\n",
        "                    for other in self.cannot_link_dict[idx]:\n",
        "                        if new_labels[other] == current_cluster:\n",
        "                            has_conflict = True\n",
        "                            break\n",
        "                    if has_conflict:\n",
        "                        break\n",
        "\n",
        "                if not has_conflict:\n",
        "                    continue\n",
        "                comp_mean = np.mean(X[indices], axis=0)\n",
        "                for new_cluster in np.argsort(np.linalg.norm(centroids - comp_mean, axis=1)):\n",
        "                    if new_cluster == current_cluster:\n",
        "                        continue\n",
        "                    valid = True\n",
        "                    for idx in indices:\n",
        "                        for other in self.cannot_link_dict[idx]:\n",
        "                            if new_labels[other] == new_cluster:\n",
        "                                valid = False\n",
        "                                break\n",
        "                        if not valid:\n",
        "                            break\n",
        "                    if valid:\n",
        "                        for idx in indices:\n",
        "                            new_labels[idx] = new_cluster\n",
        "                        changed = True\n",
        "                        break\n",
        "            self.labels_ = new_labels\n",
        "            self.cluster_centers_ = centroids\n",
        "            if not changed:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.argmin(np.linalg.norm(X[:, np.newaxis] - self.cluster_centers_, axis=2), axis=1)"
      ],
      "metadata": {
        "id": "U4Lnxp-gl4Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans с регуляризацией"
      ],
      "metadata": {
        "id": "ydDRCPhHl5F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularizedKMeans(nn.Module):\n",
        "    def __init__(self, n_clusters, n_features, must_link=[], cannot_link=[], alpha=1.0, beta=1.0):\n",
        "        super(RegularizedKMeans, self).__init__()\n",
        "        self.n_clusters = n_clusters\n",
        "        self.n_features = n_features\n",
        "        self.must_link = must_link\n",
        "        self.cannot_link = cannot_link\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        torch.manual_seed(42)\n",
        "        self.cluster_centers = nn.Parameter(torch.randn(n_clusters, n_features))\n",
        "        self.loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, X):\n",
        "        distances = torch.cdist(X, self.cluster_centers, p=2)\n",
        "        cluster_assignments = torch.argmin(distances, dim=1)\n",
        "        clustering_loss = 0.0\n",
        "        for i in range(self.n_clusters):\n",
        "            mask = (cluster_assignments == i)\n",
        "            if mask.any():\n",
        "                cluster_points = X[mask]\n",
        "                clustering_loss += self.loss_fn(cluster_points, self.cluster_centers[i].expand_as(cluster_points))\n",
        "        clustering_loss /= self.n_clusters\n",
        "        reg_loss = self.regularization_loss(distances)\n",
        "        total_loss = clustering_loss + self.alpha * reg_loss\n",
        "        return cluster_assignments, total_loss\n",
        "\n",
        "    def regularization_loss(self, distances):\n",
        "        batch_size = distances.size(0)\n",
        "        weights = F.softmax(-self.beta * distances, dim=1)\n",
        "        reg_loss = 0.0\n",
        "        must_count = 0\n",
        "        cannot_count = 0\n",
        "\n",
        "        for i, j in self.must_link:\n",
        "            if i < batch_size and j < batch_size:\n",
        "                p_i = weights[i] + 1e-8\n",
        "                p_j = weights[j] + 1e-8\n",
        "                kl_loss = F.kl_div(p_i.log(), p_j, reduction='sum') + F.kl_div(p_j.log(), p_i, reduction='sum')\n",
        "                reg_loss += kl_loss\n",
        "                must_count += 1\n",
        "\n",
        "        for i, j in self.cannot_link:\n",
        "            if i < batch_size and j < batch_size:\n",
        "                dot_product = torch.dot(weights[i], weights[j])\n",
        "                reg_loss += dot_product\n",
        "                cannot_count += 1\n",
        "        total_constraints = must_count + cannot_count\n",
        "        if total_constraints > 0:\n",
        "            reg_loss /= total_constraints\n",
        "\n",
        "        return reg_loss\n",
        "\n",
        "    def fit(self, X, epochs=100, lr=0.01):\n",
        "        X_tensor = torch.from_numpy(X).float()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            _, loss = self.forward(X_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_tensor = torch.from_numpy(X).float()\n",
        "        assignments, _ = self.forward(X_tensor)\n",
        "        return assignments.numpy()"
      ],
      "metadata": {
        "id": "is8FZ3jnl_Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans с инициализацией центроидов"
      ],
      "metadata": {
        "id": "TMCbh7yVmAmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InitializedKMeans(KMeans):\n",
        "    def __init__(self, n_clusters, init_centroids=None,\n",
        "                 n_init=5, max_iter=300, random_state=None):\n",
        "        self.init_centroids = init_centroids\n",
        "        self.n_provided = len(init_centroids) if init_centroids is not None else 0\n",
        "        if self.n_provided > 0:\n",
        "            init_centroids = np.array(init_centroids)\n",
        "            n_missing = n_clusters - self.n_provided\n",
        "            rng = np.random.RandomState(random_state)\n",
        "\n",
        "            if n_missing > 0:\n",
        "                feature_dim = init_centroids.shape[1]\n",
        "                random_centroids = rng.normal(loc=np.mean(init_centroids, axis=0),\n",
        "                                              scale=np.std(init_centroids, axis=0) + 1e-6,\n",
        "                                              size=(n_missing, feature_dim))\n",
        "                init = np.vstack([init_centroids, random_centroids])\n",
        "            else:\n",
        "                init = init_centroids\n",
        "        else:\n",
        "            init = 'k-means++'\n",
        "\n",
        "        super().__init__(n_clusters=n_clusters,\n",
        "                        init=init,\n",
        "                        n_init=n_init,\n",
        "                        max_iter=max_iter,\n",
        "                        random_state=random_state)\n",
        "\n",
        "    def fit(self, X, y=None, sample_weight=None):\n",
        "        super().fit(X, y=y, sample_weight=sample_weight)\n",
        "        if self.n_provided > 0:\n",
        "            self.cluster_centers_[:self.n_provided] = np.array(self.init_centroids)\n",
        "        return self"
      ],
      "metadata": {
        "id": "gL3Dh23wjtVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Графовые подходы"
      ],
      "metadata": {
        "id": "jUgX0-lKjx1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Графовый иерархический подход"
      ],
      "metadata": {
        "id": "qoe8YnTPlXjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConstrainedGraphClustering:\n",
        "    def __init__(self, n_clusters, must_link=[], cannot_link=[],\n",
        "                 affinity='rbf', gamma=0.1, n_neighbors=5, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.must_link = must_link\n",
        "        self.cannot_link = cannot_link\n",
        "        self.affinity = affinity\n",
        "        self.gamma = gamma\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.random_state = random_state\n",
        "        self.labels_ = None\n",
        "\n",
        "    def fit_predict(self, X):\n",
        "        if self.gamma is None and self.affinity == 'rbf':\n",
        "            self.gamma = 1.0 / (X.shape[1] * X.var()) if X.shape[1] > 0 else 0.1\n",
        "        sim_matrix = self.build_similarity_with_constraints(X)\n",
        "        distance_matrix = 1 - sim_matrix\n",
        "        np.fill_diagonal(distance_matrix, 0)\n",
        "        model = AgglomerativeClustering(n_clusters=self.n_clusters,\n",
        "                                        metric='precomputed',\n",
        "                                        linkage='average')\n",
        "        self.labels_ = model.fit_predict(distance_matrix)\n",
        "        return self.labels_\n",
        "\n",
        "    def build_similarity_with_constraints(self, X):\n",
        "        n = X.shape[0]\n",
        "        if self.affinity == 'rbf':\n",
        "            sim = rbf_kernel(X, gamma=self.gamma)\n",
        "        elif self.affinity == 'cosine':\n",
        "            sim = cosine_similarity(X)\n",
        "        elif self.affinity == 'nearest_neighbors':\n",
        "            sim = np.zeros((n, n))\n",
        "            distances = pairwise_distances(X)\n",
        "            for i in range(n):\n",
        "                neighbors = np.argsort(distances[i])[1:self.n_neighbors + 1]\n",
        "                sim[i, neighbors] = 1\n",
        "                sim[neighbors, i] = 1\n",
        "\n",
        "        for i, j in self.must_link:\n",
        "            sim[i, j] = sim[j, i] = 1.0\n",
        "        for i, j in self.cannot_link:\n",
        "            sim[i, j] = sim[j, i] = 0.0\n",
        "        return sim"
      ],
      "metadata": {
        "id": "OmT_V8TMlc-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Графовый подход с регуляризацией"
      ],
      "metadata": {
        "id": "4Fhtv7nNleXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularizedGraphClustering(nn.Module):\n",
        "    def __init__(self, n_clusters, n_nodes, must_link=[], cannot_link=[], alpha=1.0, margin=1.0):\n",
        "        super().__init__()\n",
        "        self.n_clusters = n_clusters\n",
        "        self.n_nodes = n_nodes\n",
        "        self.alpha = alpha\n",
        "        self.margin = margin\n",
        "        self.cluster_probs = nn.Parameter(torch.randn(n_nodes, n_clusters))\n",
        "        nn.init.xavier_uniform_(self.cluster_probs)\n",
        "        self.register_constraints(must_link, cannot_link)\n",
        "\n",
        "    def register_constraints(self, must_link, cannot_link):\n",
        "        if must_link:\n",
        "            self.register_buffer('ml_i', torch.tensor([i for i, _ in must_link], dtype=torch.long))\n",
        "            self.register_buffer('ml_j', torch.tensor([j for _, j in must_link], dtype=torch.long))\n",
        "        else:\n",
        "            self.ml_i = self.ml_j = None\n",
        "        if cannot_link:\n",
        "            self.register_buffer('cl_i', torch.tensor([i for i, _ in cannot_link], dtype=torch.long))\n",
        "            self.register_buffer('cl_j', torch.tensor([j for _, j in cannot_link], dtype=torch.long))\n",
        "        else:\n",
        "            self.cl_i = self.cl_j = None\n",
        "\n",
        "    def constraint_loss(self, P):\n",
        "        loss = 0.0\n",
        "        if self.ml_i is not None:\n",
        "            diff = P[self.ml_i] - P[self.ml_j]\n",
        "            loss += torch.mean(torch.sum(diff**2, dim=1))\n",
        "        if self.cl_i is not None:\n",
        "            diff = P[self.cl_i] - P[self.cl_j]\n",
        "            dist = torch.sum(diff**2, dim=1)\n",
        "            loss += torch.mean(torch.clamp(self.margin - dist, min=0.0))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, adj_matrix):\n",
        "        P = torch.softmax(self.cluster_probs, dim=1)\n",
        "        cluster_assignment = torch.argmax(P, dim=1)\n",
        "        graph_loss = self.graph_cut_loss(P, adj_matrix)\n",
        "        constraint_loss = self.constraint_loss(P)\n",
        "        total_loss = graph_loss + self.alpha * constraint_loss\n",
        "        return cluster_assignment, total_loss\n",
        "\n",
        "    def graph_cut_loss(self, P, adj_matrix):\n",
        "        intra_cluster = torch.sum(adj_matrix * (P @ P.T))\n",
        "        total_connections = torch.sum(adj_matrix)\n",
        "        cut_loss = 1 - (intra_cluster / total_connections)\n",
        "        return cut_loss\n",
        "\n",
        "    def fit(self, adj_matrix, epochs=100, lr=0.01, verbose=False):\n",
        "        adj_matrix = torch.FloatTensor(adj_matrix)\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            _, loss = self.forward(adj_matrix)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if verbose and epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    def predict(self):\n",
        "        with torch.no_grad():\n",
        "            P = torch.softmax(self.cluster_probs, dim=1)\n",
        "            return torch.argmax(P, dim=1).numpy()"
      ],
      "metadata": {
        "id": "kUH-8Y7Cj7h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Графовый подход с распространением меток"
      ],
      "metadata": {
        "id": "hd24lMTnlk73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelPropagationClustering:\n",
        "    def __init__(self, n_clusters, known_labels, n_neighbors=7, max_iter=30, tol=1e-3):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.known_labels = known_labels\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.labels_ = None\n",
        "\n",
        "    def fit_predict(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        adj_matrix = kneighbors_graph(X,\n",
        "                                      self.n_neighbors,\n",
        "                                      mode='connectivity',\n",
        "                                      include_self=True)\n",
        "        adj_matrix = adj_matrix.maximum(adj_matrix.T)\n",
        "        labels = -np.ones(n_samples, dtype=int)\n",
        "        for idx, lbl in self.known_labels.items():\n",
        "            if idx < n_samples:\n",
        "                labels[idx] = lbl\n",
        "        label_probs = np.zeros((n_samples, self.n_clusters))\n",
        "        for i in range(n_samples):\n",
        "            if labels[i] != -1:\n",
        "                label_probs[i, labels[i]] = 1.0\n",
        "        for _ in range(self.max_iter):\n",
        "            prev_probs = label_probs.copy()\n",
        "            new_probs = np.zeros_like(label_probs)\n",
        "            for i in range(n_samples):\n",
        "                if labels[i] == -1:\n",
        "                    _, neighbors = adj_matrix[i].nonzero()\n",
        "\n",
        "                    if neighbors.size > 0:\n",
        "                        new_probs[i] = np.mean(prev_probs[neighbors], axis=0)\n",
        "                    else:\n",
        "                        new_probs[i] = prev_probs[i]\n",
        "                else:\n",
        "                    new_probs[i] = prev_probs[i]\n",
        "            if np.abs(new_probs - prev_probs).sum() < self.tol:\n",
        "                break\n",
        "\n",
        "            label_probs = new_probs\n",
        "\n",
        "        self.labels_ = np.argmax(label_probs, axis=1)\n",
        "        return self.labels_"
      ],
      "metadata": {
        "id": "GvR8PwRylo3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Спектральные подходы"
      ],
      "metadata": {
        "id": "qvBb-ghTj8ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Спектральная кластеризация с прямым изменением матрицы близости"
      ],
      "metadata": {
        "id": "IQx-d2JHkrqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConstrainedSpectralClustering:\n",
        "    def __init__(self, n_clusters, must_link=[], cannot_link=[],\n",
        "                affinity='rbf', gamma=0.1, n_neighbors=7, random_state=None):\n",
        "\n",
        "        self.n_clusters = n_clusters\n",
        "        self.must_link = must_link\n",
        "        self.cannot_link = cannot_link\n",
        "        self.affinity = 'nearest_neighbors'\n",
        "        self.gamma = gamma\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.random_state = random_state\n",
        "        self.labels_ = None\n",
        "\n",
        "    def fit_predict(self, X):\n",
        "        similarity_matrix = self.build_similarity_graph(X)\n",
        "\n",
        "        for i, j in self.must_link:\n",
        "            similarity_matrix[i, j] = 1.0\n",
        "            similarity_matrix[j, i] = 1.0\n",
        "        for i, j in self.cannot_link:\n",
        "            similarity_matrix[i, j] = 0.0\n",
        "            similarity_matrix[j, i] = 0.0\n",
        "\n",
        "        spectral = SpectralClustering(n_clusters=self.n_clusters,\n",
        "                                      affinity='precomputed',\n",
        "                                      random_state=self.random_state,\n",
        "                                      assign_labels='kmeans')\n",
        "\n",
        "        self.labels_ = spectral.fit_predict(similarity_matrix)\n",
        "        return self.labels_\n",
        "\n",
        "    def build_similarity_graph(self, X):\n",
        "        if self.affinity == 'rbf':\n",
        "            return rbf_kernel(X, gamma=0.1)\n",
        "        elif self.affinity == 'cosine':\n",
        "            return cosine_similarity(X)\n",
        "        elif self.affinity == 'nearest_neighbors':\n",
        "            k = 7\n",
        "            adjacency = np.zeros((X.shape[0], X.shape[0]))\n",
        "            distances = pairwise_distances(X)\n",
        "            for i in range(X.shape[0]):\n",
        "                nearest_indices = np.argsort(distances[i])[1:k+1]\n",
        "                adjacency[i, nearest_indices] = 1\n",
        "                adjacency[nearest_indices, i] = 1\n",
        "            return adjacency"
      ],
      "metadata": {
        "id": "JmcaM5O2kq1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Спектральная кластеризация с регуляризацией"
      ],
      "metadata": {
        "id": "aTarvQX7kzuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularizedSpectralClustering(nn.Module):\n",
        "    def __init__(self, n_clusters, must_link=[], cannot_link=[], alpha=1.0, margin=1.0):\n",
        "        super(RegularizedSpectralClustering, self).__init__()\n",
        "        self.n_clusters = n_clusters\n",
        "        self.must_link = must_link\n",
        "        self.cannot_link = cannot_link\n",
        "        self.alpha = alpha\n",
        "        self.margin = margin\n",
        "        torch.manual_seed(42)\n",
        "        #self.embeddings = nn.Parameter(torch.randn(n_nodes, n_clusters))\n",
        "        self.embeddings = None\n",
        "\n",
        "    def spectral_init(self, similarity_matrix, n_components):\n",
        "        L = laplacian(similarity_matrix, normed=True)\n",
        "        eigvals, eigvecs = eigh(L)\n",
        "        X_spec = torch.from_numpy(eigvecs[:, :n_components]).float()\n",
        "        self.embeddings = nn.Parameter(X_spec)\n",
        "\n",
        "    def forward(self):\n",
        "        soft_assignments = torch.softmax(self.embeddings, dim=1)\n",
        "        cluster_assignments = torch.argmax(soft_assignments, dim=1)\n",
        "        cluster_loss = -torch.mean(torch.sum(soft_assignments * torch.log(soft_assignments + 1e-10), dim=1))\n",
        "        reg_loss = self.constraint_loss(soft_assignments)\n",
        "        total_loss = cluster_loss + self.alpha * reg_loss\n",
        "        return cluster_assignments, total_loss\n",
        "\n",
        "    def constraint_loss(self, soft_assignments):\n",
        "        loss = 0.0\n",
        "        for i, j in self.must_link:\n",
        "            loss += torch.sum((soft_assignments[i] - soft_assignments[j]) ** 2)\n",
        "        for i, j in self.cannot_link:\n",
        "            dist = torch.sum((soft_assignments[i] - soft_assignments[j]) ** 2)\n",
        "            loss += torch.clamp(self.margin - dist, min=0.0)\n",
        "        return loss\n",
        "\n",
        "    def fit(self, similarity_matrix, epochs=100, lr=0.01):\n",
        "        self.spectral_init(similarity_matrix, n_components=self.n_clusters)\n",
        "        optimizer = optim.Adam([self.embeddings], lr=lr)\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            _, loss = self.forward()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    def predict(self):\n",
        "        with torch.no_grad():\n",
        "            soft_assignments = torch.softmax(self.embeddings, dim=1)\n",
        "            return torch.argmax(soft_assignments, dim=1).numpy()"
      ],
      "metadata": {
        "id": "z1_UeogGk-Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Спектральная регуляризация с воздействием матрицей ограничений на матрицу близости"
      ],
      "metadata": {
        "id": "OJa_oT4VlAZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhj1Ns5Vre75"
      },
      "outputs": [],
      "source": [
        "class ModifiedSimilaritySpectralClustering:\n",
        "    def __init__(self, n_clusters, must_link=[], cannot_link=[],\n",
        "                affinity='rbf', gamma=0.1, constraint_strength=0.5,\n",
        "                random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.must_link = must_link\n",
        "        self.cannot_link = cannot_link\n",
        "        self.affinity = 'nearest_neighbors'\n",
        "        self.gamma = gamma\n",
        "        self.constraint_strength = constraint_strength\n",
        "        self.random_state = random_state\n",
        "        self.labels_ = None\n",
        "\n",
        "    def fit_predict(self, X):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        similarity = self.build_similarity_graph(X)\n",
        "        constraint = np.zeros_like(similarity)\n",
        "        for i, j in self.must_link:\n",
        "            if i < n_samples and j < n_samples:\n",
        "                constraint[i,j] = self.constraint_strength\n",
        "                constraint[j,i] = self.constraint_strength\n",
        "\n",
        "        for i, j in self.cannot_link:\n",
        "            if i < n_samples and j < n_samples:\n",
        "                constraint[i,j] = -self.constraint_strength\n",
        "                constraint[j,i] = -self.constraint_strength\n",
        "        modified = similarity + constraint\n",
        "        modified = np.clip(modified, 1e-8, None)\n",
        "        model = SpectralClustering(n_clusters=self.n_clusters,\n",
        "                                  affinity='precomputed',\n",
        "                                  random_state=self.random_state,)\n",
        "                                  #n_components=min(self.n_clusters*2, n_samples-1)\n",
        "        self.labels_ = model.fit_predict(modified)\n",
        "        return self.labels_\n",
        "\n",
        "    def build_similarity_graph(self, X):\n",
        "        if self.affinity == 'rbf':\n",
        "            return rbf_kernel(X, gamma=0.1)\n",
        "        elif self.affinity == 'cosine':\n",
        "            return cosine_similarity(X)\n",
        "        elif self.affinity == 'nearest_neighbors':\n",
        "            k = 7\n",
        "            adjacency = np.zeros((X.shape[0], X.shape[0]))\n",
        "            distances = pairwise_distances(X)\n",
        "            for i in range(X.shape[0]):\n",
        "                nearest_indices = np.argsort(distances[i])[1:k+1]\n",
        "                adjacency[i, nearest_indices] = 1\n",
        "                adjacency[nearest_indices, i] = 1\n",
        "            return adjacency"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Запуск экспериментов"
      ],
      "metadata": {
        "id": "AYqInO3DkNJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_algorithms(X, y, n_clusters, labeled_percentages=[0, 0.005, 0.01, 0.02, 0.05, 0.1], random_state=42):\n",
        "    results = {}\n",
        "    pca = PCA(n_components=50)\n",
        "    X_reduced = pca.fit_transform(X)\n",
        "    X_train = X_reduced\n",
        "    y_train = y\n",
        "\n",
        "    for labeled_percent in labeled_percentages:\n",
        "        results[labeled_percent] = {}\n",
        "        print(f\"\\nПроцент размеченных данных: {labeled_percent * 100}%\")\n",
        "\n",
        "        n_labeled = int(labeled_percent * len(y_train))\n",
        "        labeled_indices = np.random.choice(len(y_train), n_labeled, replace=False)\n",
        "        must_link1, cannot_link1, known_labels = create_constraints(y_train, labeled_indices)\n",
        "        for i in range (3):\n",
        "          must_link = must_link1.copy()\n",
        "          cannot_link = cannot_link1.copy()\n",
        "          if i == 1:\n",
        "            must_link = []\n",
        "          if i == 2:\n",
        "            cannot_link = []\n",
        "          print(\"Количество must-link: \", len(must_link))\n",
        "          print(\"Количество cannot-link: \", len(cannot_link))\n",
        "          # 1. K-Means\n",
        "          # a) Прямая проверка ограничений\n",
        "          ckmeans = ConstrainedKMeans(n_clusters=n_clusters, must_link=must_link, cannot_link=cannot_link, random_state=random_state)\n",
        "          ckmeans.fit(X_train)\n",
        "          y_pred_ckmeans = ckmeans.predict(X_train)\n",
        "          ari, nmi, fm = evaluate_clustering(y_train, y_pred_ckmeans)\n",
        "          results[labeled_percent]['ConstrainedKMeans'] = {'ARI': ari, 'NMI': nmi, 'FM': fm}\n",
        "          print(f\"  K-Means (Прямая проверка): ARI={ari:.3f}, NMI={nmi:.3f}, FM={fm:.3f}\")\n",
        "\n",
        "          # б) K-Means с регуляризацией\n",
        "          regularized_kmeans = RegularizedKMeans(n_clusters=n_clusters, n_features=X_train.shape[1], must_link=must_link, cannot_link=cannot_link)\n",
        "          regularized_kmeans.fit(X_train, epochs=300, lr=0.01)\n",
        "          y_pred_reg_kmeans = regularized_kmeans.predict(X_train)\n",
        "          ari, nmi, fm = evaluate_clustering(y_train, y_pred_reg_kmeans)\n",
        "          results[labeled_percent]['RegularizedKMeans'] = {'ARI': ari, 'NMI': nmi, 'FM': fm}\n",
        "          print(f\"  K-Means (Регуляризация): ARI={ari:.3f}, NMI={nmi:.3f}, FM={fm:.3f}\")\n",
        "\n",
        "          # в) K-Means с инициализацией на основе известных меток\n",
        "          X_labeled = X_train[labeled_indices]\n",
        "          known_labels_dict = {i:y_train[i] for i in labeled_indices}\n",
        "          cluster_points = defaultdict(list)\n",
        "\n",
        "          for idx in labeled_indices:\n",
        "              label = y_train[idx]\n",
        "              cluster_points[label].append(X_train[idx])\n",
        "\n",
        "          init_centroids = []\n",
        "          for label, points in sorted(cluster_points.items()):\n",
        "              init_centroids.append(np.mean(points, axis=0))\n",
        "\n",
        "          initialized_kmeans = InitializedKMeans(\n",
        "              n_clusters=n_clusters,\n",
        "              init_centroids=init_centroids,\n",
        "              random_state=random_state)\n",
        "\n",
        "          initialized_kmeans.fit(X_train)\n",
        "          y_pred_init_kmeans = initialized_kmeans.predict(X_train)\n",
        "          ari, nmi, fm = evaluate_clustering(y_train, y_pred_init_kmeans)\n",
        "          results[labeled_percent]['InitializedKMeans'] = {'ARI': ari, 'NMI': nmi, 'FM': fm}\n",
        "          print(f\"  K-Means (Инициализация): ARI={ari:.3f}, NMI={nmi:.3f}, FM={fm:.3f}\")\n",
        "\n",
        "          # 2. Графовые алгоритмы\n",
        "          # a) Учет ограничений в весах ребер\n",
        "          constrained_graph_clustering = ConstrainedGraphClustering(n_clusters=n_clusters, must_link=must_link, cannot_link=cannot_link, random_state=random_state)\n",
        "          y_pred_graph = constrained_graph_clustering.fit_predict(X_train)\n",
        "          ari, nmi, fm = evaluate_clustering(y_train, y_pred_graph)\n",
        "          results[labeled_percent]['ConstrainedGraphClustering'] = {'ARI': ari, 'NMI': nmi, 'FM': fm}\n",
        "          print(f\"  Графовый (Веса ребер): ARI={ari:.3f}, NMI={nmi:.3f}, FM={fm:.3f}\")\n",
        "\n",
        "          #б) Графовый с регуляризацией\n",
        "          similarity_matrix = constrained_graph_clustering.build_similarity_with_constraints(X_train)\n",
        "          regularized_graph_clustering = RegularizedGraphClustering(n_clusters=n_clusters, n_nodes = X_train.shape[0], must_link=must_link, cannot_link=cannot_link)\n",
        "          regularized_graph_clustering.fit(similarity_matrix, epochs=300, lr=0.01)\n",
        "          y_pred_reg_graph = regularized_graph_clustering.predict()\n",
        "          ari, nmi, fm = evaluate_clustering(y_train, y_pred_reg_graph)\n",
        "          results[labeled_percent]['RegularizedGraphClustering'] = {'ARI': ari, 'NMI': nmi, 'FM': fm}\n",
        "          print(f\"  Графовый (Регуляризация): ARI={ari:.3f}, NMI={nmi:.3f}, FM={fm:.3f}\")\n",
        "\n",
        "          # в) Распространение меток\n",
        "          label_propagation = LabelPropagationClustering(n_clusters=n_clusters, known_labels=known_labels)\n",
        "          y_pred_label_prop = label_propagation.fit_predict(X_train)\n",
        "          ari, nmi, fm = evaluate_clustering(y_train, y_pred_label_prop)\n",
        "          results[labeled_percent]['LabelPropagation'] = {'ARI': ari, 'NMI': nmi, 'FM': fm}\n",
        "          print(f\"  Распространение меток: ARI={ari:.3f}, NMI={nmi:.3f}, FM={fm:.3f}\")\n",
        "\n",
        "          #3. Спектральная кластеризация\n",
        "          #a) Учет ограничений в матрице схожести\n",
        "          constrained_spectral_clustering = ConstrainedSpectralClustering(n_clusters=n_clusters, must_link=must_link, cannot_link=cannot_link, random_state=random_state)\n",
        "          y_pred_spectral = constrained_spectral_clustering.fit_predict(X_train)\n",
        "          ari, nmi, fm = evaluate_clustering(y_train, y_pred_spectral)\n",
        "          results[labeled_percent]['ConstrainedSpectralClustering'] = {'ARI': ari, 'NMI': nmi, 'FM': fm}\n",
        "          print(f\"  Спектральная (Матрица схожести): ARI={ari:.3f}, NMI={nmi:.3f}, FM={fm:.3f}\")\n",
        "\n",
        "          # б) Спектральная кластеризация с регуляризацией\n",
        "          similarity_matrix_spectral = constrained_spectral_clustering.build_similarity_graph(X_train)\n",
        "          regularized_spectral_clustering = RegularizedSpectralClustering(n_clusters=n_clusters, must_link=must_link, cannot_link=cannot_link)\n",
        "          regularized_spectral_clustering.fit(similarity_matrix_spectral, epochs=300, lr=0.01)\n",
        "          y_pred_reg_spectral = regularized_spectral_clustering.predict()\n",
        "          ari, nmi, fm = evaluate_clustering(y_train, y_pred_reg_spectral)\n",
        "          results[labeled_percent]['RegularizedSpectralClustering'] = {'ARI': ari, 'NMI': nmi, 'FM': fm}\n",
        "          print(f\"  Спектральная (Регуляризация): ARI={ari:.3f}, NMI={nmi:.3f}, FM={fm:.3f}\")\n",
        "\n",
        "          # в) Спектральная кластеризация с модифицированной матрицей схожести\n",
        "          modified_similarity_spectral_clustering = ModifiedSimilaritySpectralClustering(n_clusters=n_clusters, must_link=must_link, cannot_link=cannot_link, random_state=random_state)\n",
        "          y_pred_modified_spectral = modified_similarity_spectral_clustering.fit_predict(X_train)\n",
        "          ari, nmi, fm = evaluate_clustering(y_train, y_pred_modified_spectral)\n",
        "          results[labeled_percent]['ModifiedSimilaritySpectralClustering'] = {'ARI': ari, 'NMI': nmi, 'FM': fm}\n",
        "          print(f\"  Спектральная (Модиф. схожесть): ARI={ari:.3f}, NMI={nmi:.3f}, FM={fm:.3f}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "n_data = [100, 600, 2500, 6000]\n",
        "#n_data = [1500]\n",
        "#n_d=1500\n",
        "for n_d in n_data:\n",
        "  data, labels = load_caltech101(data_path='/content/', n_samples=n_d)\n",
        "  torch.manual_seed(42)\n",
        "  X, y = data, labels\n",
        "  n_clusters = len(np.unique(y))\n",
        "  le = LabelEncoder()\n",
        "  y = le.fit_transform(y)\n",
        "  results, y_pred_modified_spectral = compare_algorithms(X, y, n_clusters)"
      ],
      "metadata": {
        "id": "V2CUx9dnkRyh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
